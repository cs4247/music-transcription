{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook is designed to run in Kaggle. This is done \n",
    "to offload handling/storage of the 100+GB dataset. \n",
    "The MAESTRO V2 dataset should be added as input to the notebook. \n",
    "It can be found at:\n",
    "from https://www.kaggle.com/datasets/jackvial/themaestrodatasetv2/data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac617a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from data.dataset import MaestroDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa.display\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c144b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaestroASTDataset\n",
    "class MaestroASTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For AST training: returns waveform + token_ids (LongTensor)\n",
    "    Tokenization is done from the MIDI file directly (bypasses piano-roll).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        tokenizer,                 # EventMIDITokenizer instance\n",
    "        csv_path: str | None = None,\n",
    "        year=None,\n",
    "        split: str = \"train\",\n",
    "        sr: int = 16000,\n",
    "        subset_size: int | None = None,\n",
    "        max_token_len: int = 256,\n",
    "        return_midi_path: bool = False,   # <-- add this\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sr = sr\n",
    "        self.max_token_len = max_token_len\n",
    "        self.return_midi_path = return_midi_path\n",
    "\n",
    "        if csv_path is None:\n",
    "            csv_path = os.path.join(root_dir, \"maestro-v2.0.0.csv\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if year is not None:\n",
    "            if isinstance(year, str) and \",\" in year:\n",
    "                years = [int(y.strip()) for y in year.split(\",\") if y.strip()]\n",
    "            elif isinstance(year, (list, tuple, set)):\n",
    "                years = [int(y) for y in year]\n",
    "            else:\n",
    "                years = [int(year)]\n",
    "            df = df[df[\"year\"].isin(years)]\n",
    "\n",
    "        if split is not None:\n",
    "            df = df[df[\"split\"] == split]\n",
    "\n",
    "        if subset_size:\n",
    "            df = df.head(subset_size)\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = os.path.join(self.root_dir, row[\"audio_filename\"])\n",
    "        midi_path  = os.path.join(self.root_dir, row[\"midi_filename\"])\n",
    "\n",
    "        if not os.path.exists(audio_path) and audio_path.endswith(\".wav\"):\n",
    "            audio_path = audio_path.replace(\".wav\", \".mp3\")\n",
    "\n",
    "        y, _ = librosa.load(audio_path, sr=self.sr, mono=True)\n",
    "        waveform = torch.tensor(y.astype(np.float32))\n",
    "\n",
    "        token_list = self.tokenizer.encode_midi_path(midi_path, max_len=self.max_token_len)\n",
    "        token_ids = torch.tensor(token_list, dtype=torch.long)\n",
    "\n",
    "        if self.return_midi_path:\n",
    "            return waveform, token_ids, midi_path\n",
    "        return waveform, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = MaestroDataset(\n",
    "    root_dir=\"/kaggle/input/themaestrodatasetv2/maestro-v2.0.0\",\n",
    "    year=\"2017\",\n",
    "    subset_size=5  # for quick loading\n",
    ")\n",
    "\n",
    "# Preview number of samples\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EventMIDITokenizer\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "import pretty_midi\n",
    "\n",
    "class EventMIDITokenizer:\n",
    "    \"\"\"\n",
    "    Event-based tokenizer (REMI-ish but simpler):\n",
    "      - 0: <sos>\n",
    "      - 1: <eos>\n",
    "      - 2: <pad>\n",
    "\n",
    "      - NOTE_ON(pitch):  note_on_base + pitch          (pitch 0..127)\n",
    "      - NOTE_OFF(pitch): note_off_base + pitch         (pitch 0..127)\n",
    "      - TIME_SHIFT(k):   time_shift_base + (k-1)       (k=1..max_time_shift)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int = 512, frame_rate: int = 100, max_time_shift: int = 100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.frame_rate = int(frame_rate)\n",
    "        self.max_time_shift = int(max_time_shift)\n",
    "\n",
    "        self.sos = 0\n",
    "        self.eos = 1\n",
    "        self.pad = 2\n",
    "\n",
    "        # Keep gaps for safety/readability\n",
    "        self.note_on_base = 10           # 10..137\n",
    "        self.note_off_base = 160         # 160..287\n",
    "        self.time_shift_base = 320       # 320..(320+max_time_shift-1)\n",
    "\n",
    "        # sanity\n",
    "        needed = self.time_shift_base + self.max_time_shift\n",
    "        if needed > self.vocab_size:\n",
    "            raise ValueError(f\"vocab_size too small; need >= {needed}, got {self.vocab_size}\")\n",
    "\n",
    "    def note_on_id(self, pitch: int) -> int:\n",
    "        return self.note_on_base + int(pitch)\n",
    "\n",
    "    def note_off_id(self, pitch: int) -> int:\n",
    "        return self.note_off_base + int(pitch)\n",
    "\n",
    "    def time_shift_id(self, k: int) -> int:\n",
    "        k = int(max(1, min(self.max_time_shift, k)))\n",
    "        return self.time_shift_base + (k - 1)\n",
    "\n",
    "    def is_note_on(self, tok: int) -> bool:\n",
    "        return self.note_on_base <= tok < self.note_on_base + 128\n",
    "\n",
    "    def is_note_off(self, tok: int) -> bool:\n",
    "        return self.note_off_base <= tok < self.note_off_base + 128\n",
    "\n",
    "    def is_time_shift(self, tok: int) -> bool:\n",
    "        return self.time_shift_base <= tok < self.time_shift_base + self.max_time_shift\n",
    "\n",
    "    def tok_to_pitch(self, tok: int) -> int:\n",
    "        if self.is_note_on(tok):\n",
    "            return tok - self.note_on_base\n",
    "        if self.is_note_off(tok):\n",
    "            return tok - self.note_off_base\n",
    "        raise ValueError(\"Not a pitch token\")\n",
    "\n",
    "    def tok_to_shift(self, tok: int) -> int:\n",
    "        return (tok - self.time_shift_base) + 1\n",
    "\n",
    "    def encode_midi_path(self, midi_path: str, max_len: int = 512) -> List[int]:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "        return self.encode_pretty_midi(pm, max_len=max_len)\n",
    "\n",
    "    def encode_midi(self, midi: Union[str, pretty_midi.PrettyMIDI], max_len: int = 512) -> List[int]:\n",
    "        \"\"\"\n",
    "        Backwards compatible: accept either a path or a PrettyMIDI object.\n",
    "        \"\"\"\n",
    "        if isinstance(midi, str):\n",
    "            pm = pretty_midi.PrettyMIDI(midi)\n",
    "        elif isinstance(midi, pretty_midi.PrettyMIDI):\n",
    "            pm = midi\n",
    "        else:\n",
    "            raise TypeError(f\"encode_midi expected str or PrettyMIDI, got {type(midi)}\")\n",
    "        return self.encode_pretty_midi(pm, max_len=max_len)\n",
    "\n",
    "    def encode_pretty_midi(self, pm: pretty_midi.PrettyMIDI, max_len: int = 512) -> List[int]:\n",
    "        # Collect note on/off events from ALL instruments\n",
    "        events: List[Tuple[int, int, int]] = []  # (frame, kind, pitch) kind: 0=off,1=on\n",
    "        for inst in pm.instruments:\n",
    "            if inst.is_drum:\n",
    "                continue\n",
    "            for n in inst.notes:\n",
    "                on_f = int(round(n.start * self.frame_rate))\n",
    "                off_f = int(round(n.end * self.frame_rate))\n",
    "                if off_f <= on_f:\n",
    "                    off_f = on_f + 1\n",
    "                pitch = int(n.pitch)\n",
    "                if 0 <= pitch <= 127:\n",
    "                    events.append((on_f, 1, pitch))\n",
    "                    events.append((off_f, 0, pitch))\n",
    "\n",
    "        # Sort: by time, then OFF before ON\n",
    "        events.sort(key=lambda x: (x[0], x[1]))  # kind: 0(off) then 1(on)\n",
    "\n",
    "        seq = [self.sos]\n",
    "        cur_f = 0\n",
    "\n",
    "        def emit_shift(delta: int):\n",
    "            nonlocal seq\n",
    "            while delta > 0 and len(seq) < max_len - 1:\n",
    "                k = min(self.max_time_shift, delta)\n",
    "                seq.append(self.time_shift_id(k))\n",
    "                delta -= k\n",
    "\n",
    "        for f, kind, pitch in events:\n",
    "            if len(seq) >= max_len - 1:\n",
    "                break\n",
    "\n",
    "            delta = f - cur_f\n",
    "            if delta > 0:\n",
    "                emit_shift(delta)\n",
    "                cur_f = f\n",
    "\n",
    "            if len(seq) >= max_len - 1:\n",
    "                break\n",
    "\n",
    "            seq.append(self.note_off_id(pitch) if kind == 0 else self.note_on_id(pitch))\n",
    "\n",
    "        seq.append(self.eos)\n",
    "\n",
    "        if len(seq) < max_len:\n",
    "            seq += [self.pad] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        return seq\n",
    "    \n",
    "\n",
    "    def decode_to_pretty_midi(self, tokens: List[int], out_path: str) -> str:\n",
    "        pm = pretty_midi.PrettyMIDI()\n",
    "        inst = pretty_midi.Instrument(program=0)\n",
    "\n",
    "        t_f = 0\n",
    "        active: Dict[int, int] = {}  # pitch -> start_frame\n",
    "\n",
    "        for tok in tokens:\n",
    "            tok = int(tok)\n",
    "            if tok in (self.sos, self.pad):\n",
    "                continue\n",
    "            if tok == self.eos:\n",
    "                break\n",
    "\n",
    "            if self.is_time_shift(tok):\n",
    "                t_f += self.tok_to_shift(tok)\n",
    "                continue\n",
    "\n",
    "            if self.is_note_on(tok):\n",
    "                p = self.tok_to_pitch(tok)\n",
    "                # if already active, ignore or restart; ignore is safest\n",
    "                if p not in active:\n",
    "                    active[p] = t_f\n",
    "                continue\n",
    "\n",
    "            if self.is_note_off(tok):\n",
    "                p = self.tok_to_pitch(tok)\n",
    "                if p in active:\n",
    "                    start_f = active.pop(p)\n",
    "                    start = start_f / self.frame_rate\n",
    "                    end = max((t_f / self.frame_rate), start + (1.0 / self.frame_rate))\n",
    "                    inst.notes.append(pretty_midi.Note(velocity=80, pitch=p, start=start, end=end))\n",
    "                continue\n",
    "\n",
    "        # close hanging notes\n",
    "        end_f = t_f\n",
    "        for p, start_f in active.items():\n",
    "            start = start_f / self.frame_rate\n",
    "            end = max((end_f / self.frame_rate), start + (1.0 / self.frame_rate))\n",
    "            inst.notes.append(pretty_midi.Note(velocity=80, pitch=p, start=start, end=end))\n",
    "\n",
    "        pm.instruments.append(inst)\n",
    "        pm.write(out_path)\n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription_model\n",
    "class TranscriptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    High-level wrapper for automatic music transcription models.\n",
    "    Handles:\n",
    "      - forward pass\n",
    "      - loss computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str = \"transformer\",\n",
    "        device: str = \"cpu\",\n",
    "        vocab_size: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = model_type.lower()\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        if self.model_type in [\"ast\", \"transformer\", \"audio_transformer\"]:\n",
    "            self.model = ASTModel(device=device, max_output_len=1024, remi_vocab_size=vocab_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        # --- Tokenizer (ONE instance, reused everywhere) ---\n",
    "        self.tokenizer = EventMIDITokenizer(vocab_size=self.vocab_size)\n",
    "\n",
    "        # --- Weighted CE loss (REPLACES old CrossEntropyLoss) ---\n",
    "        self.criterion = self._build_weighted_criterion()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Weighted loss (THIS replaces your old nn.CrossEntropyLoss)\n",
    "    # ------------------------------------------------------------\n",
    "    def _build_weighted_criterion(self):\n",
    "        weights = torch.ones(self.vocab_size, device=self.device)\n",
    "        \n",
    "        tok = self.tokenizer\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # TIME_SHIFT tokens: [time_shift_base, base + max_time_shift)\n",
    "        # --------------------------------------------------\n",
    "        ts_start = tok.time_shift_base\n",
    "        ts_end = min(tok.time_shift_base + tok.max_time_shift, self.vocab_size)\n",
    "        \n",
    "        weights[ts_start:ts_end] = 0.3\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # NOTE_ON tokens: [note_on_base, note_off_base)\n",
    "        # --------------------------------------------------\n",
    "        no_start = tok.note_on_base\n",
    "        no_end = min(tok.note_off_base, self.vocab_size)\n",
    "        \n",
    "        weights[no_start:no_end] = 2.0\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # Never predict SOS\n",
    "        # --------------------------------------------------\n",
    "        weights[tok.sos] = 0.0\n",
    "        weights[tok.eos] = 0.0\n",
    "\n",
    "        \n",
    "        return nn.CrossEntropyLoss(\n",
    "            weight=weights,\n",
    "            ignore_index=tok.pad,\n",
    "        )\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Forward (delegates to ASTModel)\n",
    "    # ------------------------------------------------------------\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        sampling_rate: int = 16000,\n",
    "        targets: torch.LongTensor | None = None,\n",
    "        generate_max_len: int = 256,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For AST:\n",
    "          - If targets is provided → training mode\n",
    "            returns (logits, tgt_out)\n",
    "          - If targets is None → generation mode\n",
    "            returns generated token IDs\n",
    "        \"\"\"\n",
    "        return self.model(\n",
    "            x,\n",
    "            targets=targets,\n",
    "            # generate_max_len=generate_max_len,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_audio(self, audio, max_len=256):\n",
    "        return self.model.generate_from_audio(\n",
    "            audio=audio,\n",
    "            max_len=max_len,\n",
    "            sos_token=self.tokenizer.sos,\n",
    "            eos_token=self.tokenizer.eos,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # LOSS COMPUTATION (FIXED )\n",
    "    # ------------------------------------------------------------\n",
    "    def compute_loss(self, model_output, targets=None):\n",
    "        \"\"\"\n",
    "        AST training:\n",
    "          model_output = (logits, tgt_out)\n",
    "          logits:  (B, T-1, V)\n",
    "          tgt_out: (B, T-1)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_type in [\"ast\", \"transformer\", \"audio_transformer\"]:\n",
    "            logits, tgt_out = model_output\n",
    "\n",
    "            B, T, V = logits.shape\n",
    "            assert V == self.vocab_size, \"Vocab size mismatch\"\n",
    "\n",
    "            loss = self.criterion(\n",
    "                logits.reshape(-1, V),\n",
    "                tgt_out.reshape(-1),\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        raise NotImplementedError(\"Only AST model supported here\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Prediction (generation)\n",
    "    # ------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x, sampling_rate: int = 16000, max_len: int = 256):\n",
    "        \"\"\"\n",
    "        Generates REMI tokens using autoregressive decoding.\n",
    "        \"\"\"\n",
    "        return self.forward(\n",
    "            x,\n",
    "            targets=None,\n",
    "            generate_max_len=max_len,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transcriber\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def collate_ast_tokens(batch, pad_id: int = 2):\n",
    "    \"\"\"\n",
    "    batch: list of (waveform_tensor_1d, token_ids_1d_long)\n",
    "    returns: list_of_waveforms, token_tensor (B, L)\n",
    "    \"\"\"\n",
    "    waveforms, token_seqs = zip(*batch)\n",
    "\n",
    "    # ensure tokens are Long\n",
    "    token_seqs = [t.to(torch.long) for t in token_seqs]\n",
    "\n",
    "    max_len = max(t.numel() for t in token_seqs)\n",
    "    B = len(token_seqs)\n",
    "\n",
    "    out = torch.full((B, max_len), pad_id, dtype=torch.long)\n",
    "    for i, t in enumerate(token_seqs):\n",
    "        out[i, : t.numel()] = t\n",
    "\n",
    "    return list(waveforms), out\n",
    "\n",
    "def assert_tokens_ok(token_targets: torch.Tensor, vocab_size: int, pad_id: int = 2):\n",
    "    # token_targets: (B, T)\n",
    "    if token_targets.dtype != torch.long:\n",
    "        raise TypeError(f\"token_targets must be torch.long, got {token_targets.dtype}\")\n",
    "    mn = int(token_targets.min().item())\n",
    "    mx = int(token_targets.max().item())\n",
    "    if mn < 0 or mx >= vocab_size:\n",
    "        bad = (token_targets < 0) | (token_targets >= vocab_size)\n",
    "        idx = bad.nonzero(as_tuple=False)[:10].tolist()\n",
    "        raise ValueError(f\"Token id out of range: min={mn}, max={mx}, vocab={vocab_size}. Examples idx={idx}\")\n",
    "    # optional: ensure pad is in range\n",
    "    if not (0 <= pad_id < vocab_size):\n",
    "        raise ValueError(\"pad_id out of range\")\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    step_losses = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    # handle AST vs framewise training\n",
    "    if getattr(model, \"model_type\", \"cnn_rnn\") in [\"ast\", \"transformer\", \"audio_transformer\"]:\n",
    "        for batch in progress_bar:\n",
    "            # Support variable batch structures (some collate fns may return 2- or 3-tuples)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Unpack robustly\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                waveforms, token_targets = batch\n",
    "            elif isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "                # some collates may return (waveforms, tokens, extra)\n",
    "                waveforms, token_targets, _ = batch\n",
    "            else:\n",
    "                # fallback: assume batch is a dict-like from HF datasets\n",
    "                try:\n",
    "                    waveforms = batch[0]\n",
    "                    token_targets = batch[1]\n",
    "                except Exception:\n",
    "                    raise ValueError(\"Unsupported batch format for AST training: %r\" % (type(batch),))\n",
    "\n",
    "            # Move targets to device; waveforms will be processed by model\n",
    "            token_targets = token_targets.to(device)\n",
    "\n",
    "            # tokens = token_targets[0]\n",
    "            # unique, counts = torch.unique(tokens, return_counts=True)\n",
    "            # print(dict(zip(unique.tolist(), counts.tolist())))\n",
    "\n",
    "            \n",
    "            # # Mixed-precision forward/backward\n",
    "            # with autocast('cuda'):\n",
    "            #     model_output = model(waveforms, targets=token_targets)\n",
    "            #     loss = model.compute_loss(model_output, token_targets)\n",
    "\n",
    "\n",
    "            # In train_one_epoch / evaluate, before model(waveforms,...):\n",
    "            assert_tokens_ok(token_targets, vocab_size=512, pad_id=2)\n",
    "\n",
    "            # No AMP\n",
    "            model_output = model(waveforms, targets=token_targets)\n",
    "            loss = model.compute_loss(model_output, token_targets)\n",
    "\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    print(f\"{name}: grad mean = {p.grad.abs().mean().item():.6f}\")\n",
    "                    break\n",
    "\n",
    "            step_loss = loss.item()\n",
    "            total_loss += step_loss\n",
    "            step_losses.append(step_loss)\n",
    "            progress_bar.set_postfix({\"step_loss\": f\"{step_loss:.4f}\"})\n",
    "    else:\n",
    "        for mel, roll, lengths in progress_bar:\n",
    "            mel, roll = mel.to(device), roll.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Mixed precision forward\n",
    "            with autocast('cuda'):\n",
    "                logits = model(mel)\n",
    "                loss = model.compute_loss(logits, roll, lengths)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update running totals\n",
    "            step_loss = loss.item()\n",
    "            total_loss += step_loss\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "            # Update tqdm bar dynamically\n",
    "            progress_bar.set_postfix({\"step_loss\": f\"{step_loss:.4f}\"})\n",
    "\n",
    "        # Update running totals\n",
    "        step_loss = loss.item()\n",
    "        total_loss += step_loss\n",
    "        step_losses.append(step_loss)\n",
    "\n",
    "        # Update tqdm bar dynamically\n",
    "        progress_bar.set_postfix({\"step_loss\": f\"{step_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    progress_bar.close()\n",
    "    return avg_loss, step_losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, max_batches: int = None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch_i, batch in enumerate(tqdm(dataloader, desc=\"Validation\", leave=False)):\n",
    "        if max_batches is not None and batch_i >= max_batches:\n",
    "            break\n",
    "\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "            waveforms, token_targets = batch\n",
    "        elif isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "            waveforms, token_targets, _ = batch\n",
    "        else:\n",
    "            waveforms, token_targets = batch[0], batch[1]\n",
    "\n",
    "        assert_tokens_ok(token_targets, vocab_size=512, pad_id=2)\n",
    "\n",
    "        token_targets = token_targets.to(device)\n",
    "        logits = model(waveforms, targets=token_targets)\n",
    "        loss = model.compute_loss(logits, token_targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AST Model\n",
    "\n",
    "try:\n",
    "    # Hugging Face imports (optional). If not installed, init will raise an informative error.\n",
    "    from transformers import AutoFeatureExtractor, AutoModel\n",
    "    _HF_AVAILABLE = True\n",
    "except Exception:\n",
    "    AutoFeatureExtractor = None\n",
    "    AutoModel = None\n",
    "    _HF_AVAILABLE = False\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio Spectrogram Transformer encoder + Transformer decoder for REMI token generation.\n",
    "\n",
    "    - Uses a pretrained AST encoder from Hugging Face (specified by `pretrained_model_name`).\n",
    "    - Initially freezes encoder weights by default to reduce compute.\n",
    "    - Transformer decoder generates REMI tokens autoregressively (teacher forcing during training).\n",
    "\n",
    "    Input to forward():\n",
    "      - waveforms: Tensor[B, L] or list of 1D Tensors (raw audio in float, range [-1,1])\n",
    "      - sampling_rate: int (e.g., 16000)\n",
    "      - targets (optional): LongTensor[B, T] of token ids for teacher forcing\n",
    "\n",
    "    Returns:\n",
    "      - If targets provided -> logits: Tensor[B, T, vocab_size]\n",
    "      - Else -> generated token ids: Tensor[B, gen_len]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name: str = \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "        use_mock_encoder: bool = False,\n",
    "        freeze_encoder: bool = True,\n",
    "        remi_vocab_size: int = 512,\n",
    "        decoder_layers: int = 1,    # TODO Change to ~4\n",
    "        decoder_dim: int = 256,     # TODO change to 384\n",
    "        decoder_heads: int = 4,     # TODO change to 6\n",
    "        dropout: float = 0.0,       # TODO change to 0.2\n",
    "        max_output_len: int = 1024,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.remi_vocab_size = remi_vocab_size\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.max_output_len = max_output_len\n",
    "        self.use_mock_encoder = use_mock_encoder\n",
    "\n",
    "        if self.use_mock_encoder:\n",
    "            # Build a tiny mock feature extractor + encoder for unit tests (no HF download)\n",
    "            class _MockFeatureExtractor:\n",
    "                def __init__(self, hidden_size=64):\n",
    "                    self.hidden_size = hidden_size\n",
    "\n",
    "                def __call__(self, waveforms, sampling_rate=None, return_tensors=\"pt\", padding=True):\n",
    "                    # Return a dummy tensor shaped (B, S, hidden)\n",
    "                    import numpy as _np\n",
    "                    if isinstance(waveforms, list):\n",
    "                        B = len(waveforms)\n",
    "                        max_len = max([w.shape[0] if hasattr(w, 'shape') else len(w) for w in waveforms])\n",
    "                    else:\n",
    "                        waveforms = [waveforms]\n",
    "                        B = 1\n",
    "                        max_len = waveforms[0].shape[0]\n",
    "                    S = max(1, max_len // 160)  # coarse time dimension\n",
    "                    return {\"input_values\": torch.randn(B, S, self.hidden_size)}\n",
    "\n",
    "            class _MockEncoder(nn.Module):\n",
    "                def __init__(self, hidden_size=64):\n",
    "                    super().__init__()\n",
    "                    self.config = type(\"C\", (), {\"hidden_size\": hidden_size})\n",
    "\n",
    "                def forward(self, **kwargs):\n",
    "                    x = kwargs.get(\"input_values\")\n",
    "                    # assume x is (B, S, H)\n",
    "                    return type(\"O\", (), {\"last_hidden_state\": x})\n",
    "\n",
    "            self.feature_extractor = _MockFeatureExtractor(hidden_size=decoder_dim)\n",
    "            self.encoder = _MockEncoder(hidden_size=decoder_dim)\n",
    "        else:\n",
    "            if not _HF_AVAILABLE:\n",
    "                raise ImportError(\n",
    "                    \"The ASTModel requires `transformers` to be installed. \"\n",
    "                )\n",
    "\n",
    "            # Feature extractor converts raw audio waveforms to log-mel patches expected by AST\n",
    "            self.feature_extractor = AutoFeatureExtractor.from_pretrained(pretrained_model_name)\n",
    "\n",
    "            # Pretrained AST encoder (we'll freeze its weights by default)\n",
    "            self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # encoder hidden size (from model config)\n",
    "        enc_hidden = getattr(self.encoder.config, \"hidden_size\", None)\n",
    "        if enc_hidden is None:\n",
    "            # fallback: try common attributes\n",
    "            enc_hidden = getattr(self.encoder.config, \"embed_dim\", decoder_dim)\n",
    "\n",
    "        # Freeze encoder if requested\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Project encoder features to decoder dimensionality\n",
    "        self.enc_to_dec = nn.Linear(enc_hidden, decoder_dim)\n",
    "        \n",
    "        # Decoder token embeddings + positional embeddings\n",
    "        self.token_emb = nn.Embedding(remi_vocab_size, decoder_dim)\n",
    "        self.pos_emb = nn.Embedding(max_output_len, decoder_dim)\n",
    "        \n",
    "        # Transformer decoder stack\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=decoder_dim,\n",
    "            nhead=decoder_heads,\n",
    "            dim_feedforward=decoder_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers)\n",
    "\n",
    "        # Final projection to REMI vocabulary\n",
    "        self.output_fc = nn.Linear(decoder_dim, remi_vocab_size)\n",
    "\n",
    "        # initialization helpers\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # small init for newly added heads\n",
    "        nn.init.normal_(self.enc_to_dec.weight, mean=0.0, std=0.02)\n",
    "        if self.enc_to_dec.bias is not None:\n",
    "            nn.init.zeros_(self.enc_to_dec.bias)\n",
    "        nn.init.normal_(self.output_fc.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.output_fc.bias)\n",
    "\n",
    "    # def _generate_square_subsequent_mask(self, sz: int):\n",
    "    #     # PyTorch transformer expects float mask with -inf on illegal positions\n",
    "    #     mask = torch.triu(torch.full((sz, sz), float(\"-inf\")), diagonal=1)\n",
    "    #     return mask\n",
    "\n",
    "    # def _generate_square_subsequent_mask(self, sz):\n",
    "    #     return torch.triu(\n",
    "    #         torch.ones(sz, sz), diagonal=1\n",
    "    #     ).bool()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int):\n",
    "        return torch.triu(\n",
    "            torch.full((sz, sz), float(\"-inf\")),\n",
    "            diagonal=1\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, audio, targets):\n",
    "        device = targets.device\n",
    "        B, T = targets.shape\n",
    "\n",
    "        # Convert PyTorch tensors to 1D NumPy arrays (required by HF feature extractor)\n",
    "        converted_audio = []\n",
    "        for w in audio:\n",
    "            if isinstance(w, torch.Tensor):\n",
    "                converted_audio.append(w.detach().cpu().numpy().astype(np.float32))\n",
    "            else:\n",
    "                # just ensure dtype float32\n",
    "                converted_audio.append(np.asarray(w, dtype=np.float32))\n",
    "        audio = converted_audio\n",
    "    \n",
    "        # Feature extraction\n",
    "        inputs = self.feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        for k in inputs:\n",
    "            inputs[k] = inputs[k].to(device)\n",
    "    \n",
    "        # Encoder\n",
    "        if self.freeze_encoder:\n",
    "            with torch.no_grad():\n",
    "                enc_outputs = self.encoder(**inputs)\n",
    "        else:\n",
    "            enc_outputs = self.encoder(**inputs)\n",
    "    \n",
    "        encoder_out = enc_outputs.last_hidden_state\n",
    "        \n",
    "        # encoder_out: (B, T_enc, 768)\n",
    "        encoder_out = self.enc_to_dec(encoder_out)  # now (B, T_enc, 384)\n",
    "        memory = encoder_out.transpose(0, 1)       # (T_enc, B, 384)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_in = targets[:, :-1]\n",
    "        tgt_out = targets[:, 1:]\n",
    "        T_dec = tgt_in.size(1)\n",
    "        positions = torch.arange(T_dec, device=device).unsqueeze(0)\n",
    "        tgt_emb = self.token_emb(tgt_in) + self.pos_emb(positions)\n",
    "        tgt_emb = tgt_emb * (self.decoder_dim ** 0.5)\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)\n",
    "        memory = encoder_out.transpose(0, 1)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(T_dec).to(device)\n",
    "        dec_out = self.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        dec_out = dec_out.transpose(0, 1)\n",
    "        logits = self.output_fc(dec_out)\n",
    "    \n",
    "        return logits, tgt_out\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_audio(\n",
    "        self,\n",
    "        audio,\n",
    "        max_len=256,\n",
    "        sos_token=0,\n",
    "        eos_token=None,\n",
    "    ):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        B = len(audio)\n",
    "    \n",
    "        converted_audio = []\n",
    "        for w in audio:\n",
    "            if isinstance(w, torch.Tensor):\n",
    "                converted_audio.append(w.detach().cpu().numpy().astype(np.float32))\n",
    "            else:\n",
    "                converted_audio.append(np.asarray(w, dtype=np.float32))\n",
    "        audio = converted_audio\n",
    "    \n",
    "        inputs = self.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        enc_outputs = self.encoder(**inputs)\n",
    "        encoder_out = enc_outputs.last_hidden_state\n",
    "        encoder_out = self.enc_to_dec(encoder_out)\n",
    "        memory = encoder_out.transpose(0, 1)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # Autoregressive decoding\n",
    "        # ---------------------------\n",
    "        generated = torch.full(\n",
    "            (B, 1), sos_token, dtype=torch.long, device=device\n",
    "        )\n",
    "    \n",
    "        for _ in range(max_len - 1):\n",
    "            T = generated.size(1)\n",
    "            positions = torch.arange(T, device=device).unsqueeze(0)\n",
    "    \n",
    "            tgt_emb = self.token_emb(generated) + self.pos_emb(positions)\n",
    "            tgt_emb = tgt_emb * (self.decoder_dim ** 0.5)\n",
    "            tgt_emb = tgt_emb.transpose(0, 1)\n",
    "    \n",
    "            tgt_mask = self._generate_square_subsequent_mask(T).to(device)\n",
    "            dec_out = self.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "    \n",
    "            logits = self.output_fc(dec_out[-1])  # (B, vocab)\n",
    "            next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "    \n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "            if eos_token is not None:\n",
    "                if (next_token == eos_token).all():\n",
    "                    break\n",
    "        return generated\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        memory,\n",
    "        sos_id: int = 0,\n",
    "        max_len: int = 256,\n",
    "        do_sample: bool = False,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 0,\n",
    "        mask_sos: bool = True,\n",
    "        repetition_penalty: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"Autoregressive generation from the decoder using provided encoder memory.\n",
    "\n",
    "        Backwards-compatible defaults produce the previous greedy behavior\n",
    "        (do_sample=False, mask_sos=False, repetition_penalty=0.0).\n",
    "\n",
    "        Args:\n",
    "            memory: (S, B, D) encoder memory\n",
    "            sos_id: start token id\n",
    "            max_len: max tokens to generate\n",
    "            do_sample: whether to sample from the softmax (vs argmax)\n",
    "            temperature: softmax temperature when sampling\n",
    "            top_k: if >0, restrict sampling to top_k logits\n",
    "            mask_sos: if True, forbid emitting sos token after step 0\n",
    "            repetition_penalty: float >=0. Subtracts penalty*count[token] from logits\n",
    "\n",
    "        Returns:\n",
    "            Tensor[B, L] of generated token ids\n",
    "        \"\"\"\n",
    "        device = memory.device\n",
    "        S, B, D = memory.shape\n",
    "        vocab_size = self.remi_vocab_size\n",
    "\n",
    "        generated = torch.full((B, 1), sos_id, dtype=torch.long, device=device)\n",
    "\n",
    "        # counts per batch item for repetition penalty\n",
    "        if repetition_penalty and repetition_penalty > 0.0:\n",
    "            counts = torch.zeros((B, vocab_size), dtype=torch.long, device=device)\n",
    "            # initialize with sos count\n",
    "            counts.scatter_add_(1, generated, torch.ones_like(generated, dtype=torch.long))\n",
    "        else:\n",
    "            counts = None\n",
    "\n",
    "        def top_k_logits(logits, k: int):\n",
    "            if k <= 0:\n",
    "                return logits\n",
    "            values, _ = torch.topk(logits, k)\n",
    "            min_values = values[..., -1, None]\n",
    "            return torch.where(logits < min_values, torch.full_like(logits, float(\"-1e9\")), logits)\n",
    "\n",
    "        for step in range(max_len):\n",
    "            positions = torch.arange(generated.size(1), device=device).unsqueeze(0).expand(B, -1)\n",
    "            tgt_emb = self.token_emb(generated) + self.pos_emb(positions)\n",
    "            tgt = tgt_emb.permute(1, 0, 2).contiguous()  # (T, B, D)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "            dec_out = self.decoder(tgt, memory, tgt_mask=tgt_mask)  # (T, B, D)\n",
    "            last = dec_out[-1]  # (B, D)\n",
    "            logits = self.output_fc(last)  # (B, V)\n",
    "\n",
    "            # Optionally forbid producing sos after the first position\n",
    "            if mask_sos and step > 0:\n",
    "                if 0 <= sos_id < logits.size(-1):\n",
    "                    logits[:, sos_id] = float(\"-1e9\")\n",
    "\n",
    "            # Apply repetition penalty (simple count-based subtraction)\n",
    "            if counts is not None:\n",
    "                # subtract penalty * counts (counts is integer tensor)\n",
    "                logits = logits - repetition_penalty * counts.float()\n",
    "\n",
    "            if do_sample:\n",
    "                # sampling path: apply temperature and top_k filtering\n",
    "                sample_logits = logits / max(1e-8, float(temperature))\n",
    "                if top_k > 0:\n",
    "                    sample_logits = top_k_logits(sample_logits, top_k)\n",
    "                probs = torch.softmax(sample_logits, dim=-1)\n",
    "                next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # greedy argmax\n",
    "                next_tokens = logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            # update counts if used\n",
    "            if counts is not None:\n",
    "                counts.scatter_add_(1, next_tokens, torch.ones_like(next_tokens, dtype=torch.long))\n",
    "\n",
    "            generated = torch.cat([generated, next_tokens], dim=1)\n",
    "\n",
    "        return generated[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Plot: per-epoch averaged losses\n",
    "def plot_training_curves(train_losses, val_losses, save_path):\n",
    "    \"\"\"Plot and save loss curves.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", color=\"royalblue\", linewidth=2)\n",
    "    plt.plot(val_losses, label=\"Val Loss\", color=\"tomato\", linewidth=2)\n",
    "    plt.title(\"Training and Validation Loss\", fontsize=14)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Plot: per-step losses with epoch boundaries\n",
    "def plot_step_losses(global_step_losses, num_epochs, save_path):\n",
    "    \"\"\"\n",
    "    Plot per-step training loss across all epochs.\n",
    "\n",
    "    Args:\n",
    "        global_step_losses: list of lists, where each sublist = losses for that epoch\n",
    "        num_epochs: total number of epochs (for x-axis scaling)\n",
    "        save_path: file path for saving the figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    flat_losses = np.concatenate(global_step_losses)\n",
    "    plt.plot(flat_losses, color=\"mediumseagreen\", linewidth=1.2)\n",
    "    plt.title(\"Training Loss per Step\", fontsize=14)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add vertical lines to mark epoch boundaries\n",
    "    step = 0\n",
    "    for i, epoch_losses in enumerate(global_step_losses, 1):\n",
    "        step += len(epoch_losses)\n",
    "        plt.axvline(x=step, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.text(step, plt.ylim()[1]*0.95, f\"Epoch {i}\", rotation=90,\n",
    "                 fontsize=8, color=\"gray\", va=\"top\", ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "def main(args):\n",
    "    parser = argparse.ArgumentParser(description=\"Train music transcription model\")\n",
    "    parser.add_argument(\"--root_dir\", type=str, default=\"maestro-v2.0.0\", help=\"Path to MAESTRO dataset root\")\n",
    "    parser.add_argument(\"--year\", type=str, default=None, help=\"Subset year (e.g. 2017). Deprecated: prefer --years\")\n",
    "    parser.add_argument(\"--years\", type=str, default=None, help=\"Comma-separated list of years to include (e.g. '2013,2017')\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=25, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--subset_size\", type=int, default=None, help=\"Limit dataset size for debugging\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"cnn_rnn\", help=\"Model type: cnn_rnn or ast\")\n",
    "    parser.add_argument(\"--save_every\", type=int, default=10, help=\"Save model checkpoint every N epochs\")\n",
    "    args = parser.parse_args(args)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(torch.cuda.memory_summary(device=device, abbreviated=True)) GPU memory debugging\n",
    "\n",
    "    # Output structure ---\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(\"outputs\", timestamp)\n",
    "    checkpoint_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    checkpoint_dir = os.path.join(\"/kaggle/output/checkpoints\", timestamp)\n",
    "    logs_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(checkpoint_dir, \"model_final.pth\")\n",
    "    loss_plot_path = os.path.join(logs_dir, \"loss_curve.png\")\n",
    "    step_plot_path = os.path.join(logs_dir, \"loss_per_step.png\")\n",
    "\n",
    "    # Data\n",
    "    # return_waveform = args.model_type in [\"ast\", \"transformer\", \"audio_transformer\"]\n",
    "\n",
    "    # Support multiple years via --years (comma-separated) or single --year (backwards compat)\n",
    "    years_arg = None\n",
    "    if args.years:\n",
    "        years_arg = args.years\n",
    "    elif args.year:\n",
    "        years_arg = args.year\n",
    "\n",
    "     # create tokenizer once\n",
    "    tokenizer = EventMIDITokenizer(vocab_size=512)  # whatever args you use\n",
    "    \n",
    "    train_ds = MaestroASTDataset(\n",
    "        root_dir=args.root_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        year=years_arg,\n",
    "        split=\"train\",\n",
    "        subset_size=args.subset_size,\n",
    "        max_token_len=1024,\n",
    "    )\n",
    "    \n",
    "    val_ds = MaestroASTDataset(\n",
    "        root_dir=args.root_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        year=years_arg,\n",
    "        split=\"validation\",\n",
    "        subset_size=(args.subset_size // 5 if args.subset_size else None),\n",
    "        max_token_len=1024,\n",
    "    )\n",
    "    \n",
    "    # Nicely formatted parameters.txt\n",
    "    with open(os.path.join(logs_dir, \"parameters.txt\"), \"w\") as file:\n",
    "        file.write(\"=== Training Parameters ===\\n\")\n",
    "        file.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        file.write(f\"Device: {device}\\n\\n\")\n",
    "        for k, v in vars(args).items():\n",
    "            file.write(f\"{k:>15}: {v}\\n\")\n",
    "        file.write(f\"Training dataset size: {len(train_ds)}\\n\")\n",
    "        file.write(f\"Validation dataset size: {len(val_ds)}\\n\")\n",
    "\n",
    "    # used_collate = collate_ast\n",
    "    \n",
    "    # On Kaggle kernels multiprocessing can be problematic; detect and use 0 workers there\n",
    "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None:\n",
    "        num_workers = 0\n",
    "    else:\n",
    "        num_workers = min(4, (os.cpu_count() or 1))\n",
    "    pin_memory = True if device.type == 'cuda' else False\n",
    "    \n",
    "    used_collate = partial(collate_ast_tokens, pad_id=tokenizer.pad_id if hasattr(tokenizer, \"pad_id\") else 2)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=used_collate, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=used_collate, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    model = TranscriptionModel(model_type=args.model_type, device=device)\n",
    "\n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    # Only optimize parameters that require gradients (encoder may be frozen)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        trainable_params = model.parameters()\n",
    "    optimizer = optim.AdamW(trainable_params, lr=args.lr, weight_decay=0)  # TODO change weight decay to 1e-2\n",
    "\n",
    "    # Logging\n",
    "    train_losses, val_losses = [], []\n",
    "    global_losses = []  # list of per-step lists\n",
    "    log_txt_path = os.path.join(logs_dir, \"training_log.txt\")\n",
    "\n",
    "    with open(log_txt_path, \"w\") as log_file:\n",
    "        log_file.write(f\"Training started: {timestamp}\\n\")\n",
    "        log_file.write(f\"Device: {device}\\n\")\n",
    "        log_file.write(f\"Epochs: {args.epochs}, Batch size: {args.batch_size}, LR: {args.lr}\\n\\n\")\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{args.epochs}\")\n",
    "            # print(torch.cuda.memory_summary(device=device, abbreviated=True)) #GPU memory debugging\n",
    "\n",
    "            train_loss, step_losses = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            val_loss = evaluate(model, val_loader, device, max_batches=1)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            global_losses.append(step_losses)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            log_file.write(f\"Epoch {epoch:02d}: Train={train_loss:.4f}, Val={val_loss:.4f}\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "            # Update training curve\n",
    "            plot_training_curves(train_losses, val_losses, loss_plot_path)\n",
    "            plot_step_losses(global_losses, args.epochs, step_plot_path)\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if epoch % args.save_every == 0 or epoch == args.epochs:\n",
    "                ckpt_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\")\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"Checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "        # Final save\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"\\nFinal model saved to {model_path}\")\n",
    "        log_file.write(\"\\nTraining complete.\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code -- Training\n",
    "model = main([\"--root_dir\", \"/kaggle/input/themaestrodatasetv2/maestro-v2.0.0\",\n",
    "      \"--year\", \"2017\",\n",
    "      #\"--years\", \"2004,2006,2008,2009,2011,2013,2014,2015,2017,2018\", \n",
    "      \"--epochs\", \"50\", \n",
    "      \"--batch_size\", \"1\",\n",
    "      \"--lr\", \"1e-3\",     #TODO change to 1e-4\n",
    "      \"--save_every\", \"52\",\n",
    "      \"--subset_size\", \"1\", #\"2000\",\n",
    "      \"--model_type\", \"transformer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f08b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation driver\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "def _trim_tokens(token_list, eos_id=1, pad_id=2):\n",
    "    out = []\n",
    "    for t in token_list:\n",
    "        t = int(t)\n",
    "        if t == pad_id:\n",
    "            continue\n",
    "        out.append(t)\n",
    "        if t == eos_id:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def midi_to_note_events(pm: pretty_midi.PrettyMIDI):\n",
    "    \"\"\"Return list of (pitch, onset_sec, offset_sec) across non-drum instruments.\"\"\"\n",
    "    notes = []\n",
    "    for inst in pm.instruments:\n",
    "        if inst.is_drum:\n",
    "            continue\n",
    "        for n in inst.notes:\n",
    "            notes.append((int(n.pitch), float(n.start), float(n.end)))\n",
    "    # sort by onset for stable matching\n",
    "    notes.sort(key=lambda x: (x[1], x[0]))\n",
    "    return notes\n",
    "\n",
    "def note_f1(pred_notes, ref_notes, onset_tol=0.05, pitch_tol=0):\n",
    "    \"\"\"\n",
    "    Simple greedy matching:\n",
    "      match if |onset_pred - onset_ref| <= onset_tol and |pitch_pred - pitch_ref| <= pitch_tol\n",
    "    \"\"\"\n",
    "    used = np.zeros(len(ref_notes), dtype=bool)\n",
    "    tp = 0\n",
    "\n",
    "    for (pp, ps, pe) in pred_notes:\n",
    "        best_j = -1\n",
    "        best_dt = None\n",
    "        for j, (rp, rs, re) in enumerate(ref_notes):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            if abs(pp - rp) > pitch_tol:\n",
    "                continue\n",
    "            dt = abs(ps - rs)\n",
    "            if dt <= onset_tol:\n",
    "                if best_dt is None or dt < best_dt:\n",
    "                    best_dt = dt\n",
    "                    best_j = j\n",
    "        if best_j >= 0:\n",
    "            used[best_j] = True\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(pred_notes) - tp\n",
    "    fn = len(ref_notes) - tp\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_one_sample(model, tokenizer, waveform, gt_token_ids, out_dir=\"/kaggle/working\",\n",
    "                        max_len=1024, onset_tol=0.1, pitch_tol=0.5):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure waveform is CPU tensor (your generate_from_audio converts -> numpy internally)\n",
    "    wav = waveform.detach().cpu()\n",
    "\n",
    "    # Generate\n",
    "    gen = model.generate_from_audio(audio=[wav], max_len=max_len)\n",
    "    gen_tokens = gen[0].detach().cpu().tolist()\n",
    "\n",
    "    gt_tokens = gt_token_ids.detach().cpu().tolist()\n",
    "\n",
    "    gen_tokens = _trim_tokens(gen_tokens, eos_id=tokenizer.eos, pad_id=tokenizer.pad)\n",
    "    gt_tokens  = _trim_tokens(gt_tokens,  eos_id=tokenizer.eos, pad_id=tokenizer.pad)\n",
    "\n",
    "    # Decode both to MIDI\n",
    "    gen_mid_path = os.path.join(out_dir, \"gen.mid\")\n",
    "    gt_mid_path  = os.path.join(out_dir, \"gt.mid\")\n",
    "\n",
    "    tokenizer.decode_to_pretty_midi(gen_tokens, gen_mid_path)\n",
    "    tokenizer.decode_to_pretty_midi(gt_tokens,  gt_mid_path)\n",
    "\n",
    "    # Load and score\n",
    "    gen_pm = pretty_midi.PrettyMIDI(gen_mid_path)\n",
    "    gt_pm  = pretty_midi.PrettyMIDI(gt_mid_path)\n",
    "\n",
    "    gen_notes = midi_to_note_events(gen_pm)\n",
    "    gt_notes  = midi_to_note_events(gt_pm)\n",
    "\n",
    "    metrics = note_f1(gen_notes, gt_notes, onset_tol=onset_tol, pitch_tol=pitch_tol)\n",
    "\n",
    "    return {\n",
    "        \"gen_mid_path\": gen_mid_path,\n",
    "        \"gt_mid_path\": gt_mid_path,\n",
    "        \"n_gen_notes\": len(gen_notes),\n",
    "        \"n_gt_notes\": len(gt_notes),\n",
    "        **metrics\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Example usage\n",
    "# -----------------------\n",
    "ds = MaestroASTDataset(root_dir=\"/kaggle/input/themaestrodatasetv2/maestro-v2.0.0\",\n",
    "    split=\"train\",\n",
    "    year=\"2017\",\n",
    "    subset_size=1,\n",
    "    tokenizer=tok, \n",
    "    max_token_len=1024)\n",
    "waveform, token_ids = ds[0]\n",
    "metrics = evaluate_one_sample(model, tok, waveform, token_ids, max_len=1024)\n",
    "print(metrics)\n",
    "print(\"GT MIDI:\", metrics[\"gt_mid_path\"])\n",
    "print(\"GEN MIDI:\", metrics[\"gen_mid_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working sheet music display\n",
    "\n",
    "# --- Install MuseScore so music21 can render sheet music ---\n",
    "!apt-get update -qq\n",
    "!apt-get install -y musescore -qq\n",
    "\n",
    "# --- Import libraries ---\n",
    "from music21 import converter, environment\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Tell music21 where MuseScore is\n",
    "us = environment.UserSettings()\n",
    "us['musescoreDirectPNGPath'] = '/usr/bin/mscore'\n",
    "us['musicxmlPath'] = '/usr/bin/mscore'\n",
    "\n",
    "# --- Load MIDI file ---\n",
    "midi_path = \"/kaggle/working/generated_from_audio.mid\"\n",
    "# midi_path = \"/kaggle/input/themaestrodatasetv2/maestro-v2.0.0/2004/MIDI-Unprocessed_SMF_16_R1_2004_01-08_ORIG_MID--AUDIO_16_R1_2004_13_Track13_wav.midi\"\n",
    "score = converter.parse(midi_path)\n",
    "\n",
    "# --- Convert to PNG sheet music ---\n",
    "png_path = score.write('musicxml.png')\n",
    "\n",
    "# --- Display the resulting sheet music image ---\n",
    "display(Image(filename=png_path))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
